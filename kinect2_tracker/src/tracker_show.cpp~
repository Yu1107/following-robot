#include <ros/ros.h>
#include <iostream>
#include <ros/console.h>
#include <array>

// OpenCV Header
#include "opencv2/opencv.hpp"
#include <opencv2/core/core.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/imgproc/imgproc.hpp>
#include </home/lab5254/vision_opencv/cv_bridge/include/cv_bridge/cv_bridge.h>


// o1. OpenNI Header
#include <OpenNI.h> // /usr/include/openni2
// n1. NiTE Header
#include <NiTE.h>

#include "geometry_msgs/Point.h"
#include "sensor_msgs/Image.h"
#include <image_transport/image_transport.h>
#include <sensor_msgs/image_encodings.h>
#include <std_msgs/Bool.h>
#include <geometry_msgs/Twist.h>
#include <tf/transform_broadcaster.h>
#include <tf/transform_listener.h>

// namespace
using namespace std;
using namespace openni;
using namespace nite;


float meter = 1000.0;
bool stop_following = false;
bool following = false;
float Gestures;
float rotation = -9.557/57.3;
int Mirroring = -1;
int lost,mancount;
float bodies_x[3];
float bodies_y[3];

geometry_msgs::Point position,position_r,position_l,realPos,PDR;
cv::Point2f aPoint[15];
std_msgs::Bool goalok,shortok;

int main( int argc, char **argv )
{

    ros::init(argc, argv, "tracker_show");
    ros::NodeHandle nh, nh_priv("~");

    // Frame broadcaster
    tf::TransformBroadcaster tfBroadcast_;

    //ros Publisher
    ros::Publisher position_1_pub = nh.advertise<geometry_msgs::Point>("/tracker/position", 1);
    ros::Publisher cmd_pub = nh.advertise<geometry_msgs::Twist>("cmd_vel", 1);
    ros::Publisher right_foot_pub = nh.advertise<geometry_msgs::Point>("/right_knee/position", 1);
    ros::Publisher left_foot_pub = nh.advertise<geometry_msgs::Point>("/left_knee/position", 1);
    ros::Publisher body_tracking_position_pub = nh.advertise<geometry_msgs::Point>("tracker_body_position", 1);
    ros::Publisher PDR_pub = nh.advertise<geometry_msgs::Point>("PDR_position", 1);

    ros::Publisher goal_pub = nh.advertise<std_msgs::Bool>("/start_IMU", 1);
    ros::Publisher short_pub = nh.advertise<std_msgs::Bool>("/short_kinect", 1);

    image_transport::ImageTransport it_(nh);
    image_transport::Publisher image_pub_ = it_.advertise("/kinect_rgb", 1);

    // o2. Initial OpenNI
    OpenNI ::initialize();

    // o3. Open Device
    Device   mDevice;
    mDevice.open (ANY_DEVICE);
    // Listening to the device connect
    //printf("%s Opened, Completed.\r\n", mDevice.getDeviceInfo().getName());

    // create depth stream
    VideoStream mDepthStream;
    mDepthStream.create( mDevice, SENSOR_DEPTH );

    /*const openni::Array<VideoMode> *supportedVideoModes = &(mDepthStream.getSensorInfo().getSupportedVideoModes());
    int numOfVideoModes = supportedVideoModes->getSize();
	for (int i = 0; i < numOfVideoModes; i++){
	 VideoMode vm = (*supportedVideoModes)[i];
	 printf("%c. %dx%d at %dfps with %d format \r\n",
	 'a' + i,
	 vm.getResolutionX(),
	 vm.getResolutionY(),
	 vm.getFps(),
	 vm.getPixelFormat());
	 }*/



    // o4a. set video mode
    VideoMode mDMode;
    mDMode.setResolution( 640, 480 );
    mDMode.setFps( 30 );
    mDMode.setPixelFormat( PIXEL_FORMAT_DEPTH_1_MM );
    mDepthStream.setVideoMode( mDMode);
    mDepthStream.setMirroringEnabled(true);

    int iMaxDepth = mDepthStream.getMaxPixelValue();

    // o5. Create color stream
    VideoStream mColorStream;
    mColorStream.create( mDevice, SENSOR_COLOR );

    // o5a. set video mode
    VideoMode mCMode;
    mCMode.setResolution( 640, 480 );
    mCMode.setFps( 30 );
    mCMode.setPixelFormat( PIXEL_FORMAT_RGB888 );
    mColorStream.setVideoMode( mCMode);
    mColorStream.setMirroringEnabled(true);


    // o6. image registration
    mDevice.setImageRegistrationMode( IMAGE_REGISTRATION_DEPTH_TO_COLOR );

    // Initial NiTE
    NiTE::initialize();

    // n3. create user tracker
    UserTracker mUserTracker;
    mUserTracker.create( &mDevice );

    /*HandTracker hTracker;
    nite::Status status = hTracker.create(&mDevice);
    hTracker.startGestureDetection(nite::GESTURE_HAND_RAISE);*/

    // create OpenCV Window
    //cv::namedWindow("User Image", 0); // the size of window can be adjusted.

    // p1. start
    mColorStream.start ();
    mDepthStream.start ();

    //setvbuf(stdout, NULL, _IOLBF,0);
    ros::Rate r(50);
    while (ros::ok())
    {
        // main loop
        // p2. prepare background
        cv::Mat cImageBGR,mScaledDepth;

        // p2a. get color frame,depth frame
        VideoFrameRef mColorFrame,mDepthFrame;
        mColorStream.readFrame( &mColorFrame );


        // p2b. convert data to OpenCV format
        const  cv::Mat mImageRGB( mColorFrame.getHeight(), mColorFrame.getWidth(),
                CV_8UC3 , ( void *)mColorFrame.getData() );

        // p2c. convert form RGB to BGR
        cv::cvtColor( mImageRGB, cImageBGR, CV_RGB2BGR );
        mColorFrame.release();
        //cv::imshow( "Color Image", cImageBGR );


        //////////////////////////////////////////////// get gesture data////////////////////////////////////////////////
        /*HandTrackerFrameRef newFrame;
        hTracker.readFrame(&newFrame);

        const nite::Array<nite::GestureData>& gestures = newFrame.getGestures();
        for (int i = 0; i < gestures.getSize(); ++i){
            printf("%s Gesture Detected @ %g,%g,%g - %s \r\n", (gestures[i].getType() == nite::GESTURE_CLICK) ? "Click" : ((gestures[i].getType() == nite::GESTURE_HAND_RAISE) ? "Hand Raise" : "Wave"),
                    gestures[i].getCurrentPosition().x,
                    gestures[i].getCurrentPosition().y,
                    gestures[i].getCurrentPosition().z,
                    (gestures[i].isInProgress()) ? "In Progress" :
                        ((gestures[i].isComplete()) ? "Completed" :
                            "Initializing"));

                            if (gestures[0].getType() == nite::GESTURE_HAND_RAISE)
                            {
                                stop_following = true;
                            }
        }*/

        //////////////////////////////////////////////////  get all users frame////////////////////////////////////////////////
        UserTrackerFrameRef   mUserFrame;
        mUserTracker.readFrame( &mUserFrame );
        mUserTracker.setSkeletonSmoothingFactor( 0.3f );

        // p4. get all users data
        const nite:: Array < UserData >& aUsers = mUserFrame.getUsers();

        //ROS_INFO_STREAM("Found  user" << aUsers.getSize());
        mancount = aUsers.getSize();

        // does same tasks for every user
        // If you want the task is different for each user, the task should related to i
        for ( int i = 0; i < mancount ; ++ i ){
            // assign an id for i-th user
            const  UserData & rUser = aUsers[i];

            if (!mUserTracker.startSkeletonTracking( rUser.getId () ))
            continue;


            // p4a. check i-th user status
            /*if ( rUser.isNew () ){
                // start tracking for new user
                mUserTracker.startSkeletonTracking( rUser.getId () );
                //hTracker.startGestureDetection(nite::GESTURE_HAND_RAISE);
                ROS_INFO_STREAM("Found a new user.");
            }
            /*else if (rUser.isLost()){
               cout << "User "<<rUser.getId() <<" lost." << endl;

               lost = rUser.getId();
            }

             else if(rUser.isVisible()){

                // get user skeleton
                const  Skeleton & rSkeleton = rUser.getSkeleton();
                if (rSkeleton.getState() == nite::SKELETON_TRACKED)
                {
                    //ROS_INFO_STREAM("Now tracking user " << rUser.getId());

                    // get joints
                    const nite::SkeletonJoint& rmass = rSkeleton.getJoint( nite::JOINT_TORSO);
                    const nite::Point3f& Position = rmass.getPosition();
                    bodies_x[i] = Position.x/meter;
                    bodies_y[i] = Position.y/meter;
                    ROS_INFO_STREAM("user: " << rUser.getId() <<"(" <<bodies_x[i] << "," << bodies_y[i]<< ")");
                    
                    if (bodies_x[i] == 0 && bodies_y[i] == 0){
                      mancount = aUsers.getSize()-1;
                    }

                    //const nite::Point3f& LHPosition = rSkeleton.getJoint(nite::JOINT_LEFT_HAND).getPosition();
                    const nite::Point3f& RkPosition = rSkeleton.getJoint(nite::JOINT_RIGHT_KNEE).getPosition();
                    const nite::Point3f& LkPosition = rSkeleton.getJoint(nite::JOINT_LEFT_KNEE).getPosition();

                    //const nite::Point3f& LHIPPosition = rSkeleton.getJoint( nite::JOINT_LEFT_HIP).getPosition();
                    //const nite::Point3f& RHIPPosition = rSkeleton.getJoint( nite::JOINT_RIGHT_HIP).getPosition();
                    //const nite::Point3f& LFPosition = rSkeleton.getJoint( nite::JOINT_LEFT_FOOT).getPosition();


                    /*float uv = (RSPosition.x-LEPosition.x)*(RWPosition.x-LEPosition.x)+(RSPosition.y-LEPosition.y)*(RWPosition.y-LEPosition.y)+(RSPosition.z-LEPosition.z)*(RWPosition.z-LEPosition.z);
                     float u_v = sqrt(pow(RSPosition.x-LEPosition.x,2)+pow(RSPosition.y-LEPosition.y,2)+pow(RSPosition.z-LEPosition.z,2))*sqrt(pow(RWPosition.x-LEPosition.x,2)+pow(RWPosition.y-LEPosition.y,2)+pow(RWPosition.z-LEPosition.z,2));
                     Gestures = acos(uv/u_v)*57.3;
                     ROS_INFO("angle: %5.2f",Gestures);
                     
                     if (70 > Gestures && Gestures < 110 && stop_following == true){
                      stop_following = false;
                     following = true;
                     }


                    if(i==0 && rmass.getPositionConfidence() > 0.3 ){

                        following = false;
                        goalok.data = false;
                        goal_pub.publish(goalok);   //publish -> matlab

                        shortok.data = false;
                        short_pub.publish(shortok); //publish -> marker.cpp

                        position.x =  Position.x/meter;
                        position.y = (sin(rotation)*Position.z/meter)+(cos(rotation)*Position.y/meter);
                        position.z = (cos(rotation)*Position.z/meter)-(sin(rotation)*Position.y/meter);

                        position_r.x = RkPosition.x/meter;
                        position_r.y = RkPosition.y/meter;
                        position_r.z = RkPosition.z/meter;

                        position_l.x = LkPosition.x/meter;
                        position_l.y = LkPosition.y/meter;
                        position_l.z = LkPosition.z/meter;

                        if (stop_following != true ){
                            //ROS_INFO("user %d,skeleton[%5.2f,%5.2f,%5.2f]",rUser.getId(),position.x,position.y,position.z);

                            //convert joint position to 3D
                            realPos.x = position.z;
                            realPos.y = Mirroring*position.x;
                            realPos.z = 0;
                            body_tracking_position_pub.publish(realPos);  //publish -> marker.cpp

                            // Publish the joints(torso) over the TF stream
                            tf::Vector3 currentVec3 = tf::Vector3(realPos.x, realPos.y, 0);
                            tf::Transform transform;
                            tf::Quaternion q;
                            q.setRPY(0, 0, -1.57);
                            transform.setOrigin(currentVec3);
                            transform.setRotation(q);
                            tfBroadcast_.sendTransform(tf::StampedTransform(transform, ros::Time::now(), "kinect_link", "kinect_body"));

                        }
                            right_foot_pub.publish(position_r);
                            left_foot_pub.publish(position_l);
                            position_1_pub.publish(position);  //publish -> skeleton_follower.pys
                    }*/

                    //  build joints array
                    SkeletonJoint aJoints[15];
                    /*aJoints[ 0] = rSkeleton.getJoint( JOINT_HEAD );
                    aJoints[ 1] = rSkeleton.getJoint( JOINT_NECK );*/
                    aJoints[ 2] = rSkeleton.getJoint( JOINT_LEFT_SHOULDER );
                    aJoints[ 3] = rSkeleton.getJoint( JOINT_RIGHT_SHOULDER );
                    /*aJoints[ 4] = rSkeleton.getJoint( JOINT_LEFT_ELBOW );
                    aJoints[ 5] = rSkeleton.getJoint( JOINT_RIGHT_ELBOW );
                    aJoints[ 6] = rSkeleton.getJoint( JOINT_LEFT_HAND );
                    aJoints[ 7] = rSkeleton.getJoint( JOINT_RIGHT_HAND );*/
                    aJoints[ 8] = rSkeleton.getJoint( JOINT_TORSO );
                    aJoints[ 9] = rSkeleton.getJoint( JOINT_LEFT_HIP );
                    aJoints[10] = rSkeleton.getJoint( JOINT_RIGHT_HIP );
                    /*aJoints[11] = rSkeleton.getJoint( JOINT_LEFT_KNEE );
                      aJoints[12] = rSkeleton.getJoint( JOINT_RIGHT_KNEE );
                      aJoints[13] = rSkeleton.getJoint( JOINT_LEFT_FOOT );
                      aJoints[14] = rSkeleton.getJoint( JOINT_RIGHT_FOOT );*/

                    // convert joint position to image
                    for ( int s = 1; s < 11; ++ s ){
                        const  Point3f & rPos = aJoints[s].getPosition();
                        mUserTracker.convertJointCoordinatesToDepth(
                        rPos.x, rPos.y, rPos.z,
                        &(aPoint[s].x), &(aPoint[s].y) );
                    }

                    // draw users skeleton
                    if(i==0 && rmass.getPositionConfidence() > 0.1){
                        cv::line( cImageBGR, aPoint[ 2], aPoint[ 3], cv::Scalar ( 0, 255, 0 ), 5 );
                        cv::line( cImageBGR, aPoint[ 3], aPoint[ 10], cv::Scalar ( 0, 255, 0 ), 5 );
                        cv::line( cImageBGR, aPoint[ 10], aPoint[ 9], cv::Scalar ( 0, 255, 0 ), 5 );
                        cv::line( cImageBGR, aPoint[ 9], aPoint[ 2], cv::Scalar ( 0, 255, 0 ), 5 );
                    }
                    else if(i!=0 && rmass.getPositionConfidence() > 0.1){
                        cv::line( cImageBGR, aPoint[ 2], aPoint[ 8], cv::Scalar ( 0, 0, 255 ), 5 );
                        cv::line( cImageBGR, aPoint[ 3], aPoint[ 8], cv::Scalar ( 0, 0, 255 ), 5 );
                        cv::line( cImageBGR, aPoint[ 8], aPoint[ 9], cv::Scalar ( 0, 0, 255 ), 5 );
                        cv::line( cImageBGR, aPoint[ 10], aPoint[ 8], cv::Scalar ( 0, 0, 255 ), 5 );
                    }

                    break;
                }
            }
	        /*else {
                if (i < 1 && following == false){
                    goalok.data = true;
                    goal_pub.publish(goalok);   //publish -> odom_out_and_back.py

                    shortok.data = true;
                    short_pub.publish(shortok);    //publish -> marker.cpp

                    ROS_INFO_STREAM("Robot stop movement");
                    geometry_msgs::Twist cmd;
                    cmd.linear.x = 0;
                    cmd.angular.z = 0;
                    cmd_pub.publish(cmd);  //publish -> drrobot_player.cpp
                    following = true;


                    cout << "User "<<rUser.getId() <<" lost." << endl;
                    lost = rUser.getId();

		            PDR.x = realPos.x;
                    PDR.y = realPos.y;
                    PDR_pub.publish(PDR);

            }
        }//for*/


        /*int n,p,m,n_pos;
        float d[3];
        float Min;
        if (following == true && mancount > 1 ){

        //float Min = bodies[0][0];
        for (n=1;n < mancount;n++){

            d[n]=sqrt((pow(bodies_x[n]-bodies_x[0],2)) + (pow(bodies_y[n]-bodies_y[0],2)));


        }

        for (m=1; m < n; n++){

            float Min =min(d[n],d[n-1]);
            n_pos = n;
        }
        cout <<  "nMin "<< n_pos <<"is :" << Min  << endl;
        i=0;

        }*/

    }//rUser.isVisible()



        /*for ( int   k = 1; k <5;k++ ){  //512*424pixels
            int kk = 102.4;
            cv::line( cImageBGR, cv::Point(k*kk,53), cv::Point(k*kk,371), cv::Scalar ( 255, 0, 0 ), 2.5 );
             //cv::line( cImageBGR, cv::Point(k*kk,0), cv::Point(k*kk,480), cv::Scalar ( 255, 0, 0 ), 2.5 );
        }*/





        //////////////////////  show image
        //cv::imshow( "User Image" , cImageBGR );
        //sensor_msgs::ImagePtr msg = cv_bridge::CvImage(std_msgs::Header(), "mono16", mScaledDepth).toImageMsg();  // CV_8UC1, grayscale image
        sensor_msgs::ImagePtr msg = cv_bridge::CvImage(std_msgs::Header(), "bgr8",cImageBGR).toImageMsg(); //CV_8UC3, color image with blue-green-red color order
        image_pub_.publish(msg);


        //  check keyboard
        if ( cv ::waitKey( 1 ) == 'q' )

            ros::spinOnce();
        r.sleep();
        //fflush(stdout);
    }

    // stop
    mUserTracker.destroy();
    mColorStream.destroy();
    mDepthStream.destroy();
    mDevice.close ();
    NiTE ::shutdown();
    OpenNI ::shutdown();

    return 0;

}
