#include <ros/ros.h>
#include  <iostream>

// OpenCV Header
#include "opencv2/opencv.hpp"
#include <opencv2/core/core.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <opencv2/imgproc/imgproc.hpp>
#include </home/lab5254/vision_opencv/cv_bridge/include/cv_bridge/cv_bridge.h>


// o1. OpenNI Header
#include <OpenNI.h> // /usr/include/openni2
// n1. NiTE Header
#include <NiTE.h>

#include "geometry_msgs/Point.h"
#include "sensor_msgs/Image.h"
#include <image_transport/image_transport.h>
#include <sensor_msgs/image_encodings.h>
#include <std_msgs/Bool.h>


// namespace
using namespace std;
using namespace openni;
using namespace nite;

// using geometry_msgs::Point;

namespace enc = sensor_msgs::image_encodings;



int main( int argc, char **argv )
{
        ros::init(argc, argv, "human_tracker_show");
    ros::NodeHandle nh, nh_priv("~");


    ros::Publisher position_1_pub = nh.advertise<geometry_msgs::Point>("/tracker/position", 1);
    ros::Publisher bool_pub = nh.advertise<std_msgs::Bool>("/startIMU", 1);
    //ros::Subscriber achieved_sub = nh.subscribe<std_msgs::Bool>("goal_achieved", 1, callback);;
    ros::Publisher right_foot_pub = nh.advertise<geometry_msgs::Point>("/right_foot/position", 1);
    ros::Publisher left_foot_pub = nh.advertise<geometry_msgs::Point>("/left_foot/position", 1);

    image_transport::ImageTransport it_(nh);
    image_transport::Publisher image_pub_ = it_.advertise("/kinect_rgb", 1);
    cv_bridge::CvImagePtr cv_ptr(new cv_bridge::CvImage);

    // o2. Initial OpenNI
    OpenNI ::initialize();




    // o3. Open Device
    Device   mDevice;
    mDevice.open (ANY_DEVICE);

    // o4. create depth stream
    VideoStream mDepthStream;
    mDepthStream.create( mDevice, SENSOR_DEPTH );

    // o4a. set video mode
    VideoMode mDMode;
    mDMode.setResolution( 640, 480 );
    mDMode.setFps( 30 );
    mDMode.setPixelFormat( PIXEL_FORMAT_DEPTH_1_MM );
    mDepthStream.setVideoMode( mDMode);

    // o5. Create color stream
    VideoStream mColorStream;
    mColorStream.create( mDevice, SENSOR_COLOR );

    // o5a. set video mode
    VideoMode mCMode;
    mCMode.setResolution( 640, 480 );
    mCMode.setFps( 30 );
    mCMode.setPixelFormat( PIXEL_FORMAT_RGB888 );
    mColorStream.setVideoMode( mCMode);

    // o6. image registration
    mDevice.setImageRegistrationMode( IMAGE_REGISTRATION_DEPTH_TO_COLOR );


    // n2. Initial NiTE
    NiTE::initialize();

    // n3. create user tracker
    UserTracker mUserTracker;
    mUserTracker.create( &mDevice );
    mUserTracker.setSkeletonSmoothingFactor( 0.3f );

    // create OpenCV Window
    //cv::namedWindow("User Image", CV_WINDOW_NORMAL); // the size of window can be adjusted.

    // p1. start
    //mColorStream.start ();
    //mDepthStream.start ();

    ros::Rate r(10);
    while (ros::ok())
    {
        // main loop

        // p2. prepare background
        cv::Mat cImageBGR;

        // p2a. get color frame
        VideoFrameRef mColorFrame;
        mColorStream.readFrame( &mColorFrame );

        // p2b. convert data to OpenCV format
        const  cv::Mat mImageRGB( mColorFrame.getHeight(), mColorFrame.getWidth(),
                CV_8UC3 , ( void *)mColorFrame.getData() );
        // p2c. convert form RGB to BGR
        cv::cvtColor( mImageRGB, cImageBGR, CV_RGB2BGR );
        mColorFrame.release();

        // p3. get all users frame
        UserTrackerFrameRef   mUserFrame;
        mUserTracker.readFrame( &mUserFrame );

        // p4. get all users data
        const nite:: Array < UserData >& aUsers = mUserFrame.getUsers();

        // does same tasks for every user
        // If you want the task is different for each user, the task should related to i
        for ( int i = 0; i < aUsers.getSize(); ++ i )
        {
            // assign an id for i-th user
            const  UserData & rUser = aUsers[i];

            // p4a. check i-th user status
            if ( rUser.isNew () )
            {
                // start tracking for new user
                mUserTracker.startSkeletonTracking( rUser.getId () );
                ROS_INFO_STREAM("Found a new user.");
            }

            if ( rUser.isVisible() )
            {
                // p4b. get user skeleton
                const  Skeleton & rSkeleton = rUser.getSkeleton();
                if ( rSkeleton.getState() == SKELETON_TRACKED )
                {
                    ROS_INFO_STREAM("Now tracking user " << rUser.getId());
                    //get joints
                    const nite::SkeletonJoint& rmass = rSkeleton.getJoint( nite::JOINT_TORSO);  // origin: JOINT_TORSO
                    const nite::Point3f& Position = rmass.getPosition();
                    //const nite::Point3f& hPosition = rSkeleton.getJoint( nite::JOINT_HEAD).getPosition();


                    const nite::Point3f& rPosition = rSkeleton.getJoint(nite::JOINT_RIGHT_FOOT).getPosition();
                    const nite::Point3f& lPosition = rSkeleton.getJoint(nite::JOINT_LEFT_FOOT).getPosition();

                    cout << "[" << rUser.getId() << "]" << " > " << rPosition.x << "/" << rPosition.y << "/" << rPosition.z;
                    cout << " (" << rmass.getPositionConfidence() << ")" << endl;

                    geometry_msgs::Point position,position_r,position_l;
                    std_msgs::Bool bo0l_imu;
                    // pub the first user's position
                    if(i==0 && rmass.getPositionConfidence()>0.3) //確認該關節位置的可靠度(0 ~ 1 之間)
                    {

                        position.x = Position.x/1000;
                        position.y = Position.y/1000;
                        position.z = Position.z/1000;


                        position_r.x = rPosition.x/1000;
                        position_r.y = rPosition.y/1000;
                        position_r.z = rPosition.z/1000;

                        position_l.x = lPosition.x/1000;
                        position_l.y = lPosition.y/1000;
                        position_l.z = lPosition.z/1000;

                        right_foot_pub.publish(position_r);
                        left_foot_pub.publish(position_l);
                        position_1_pub.publish(position);



                        /*bo0l_imu.data = false;
                         * bool_pub.publish(bo0l_imu);
                         * if (position.z >= 3)
                         * {
                         * bo0l_imu.data = true;
                         * bool_pub.publish(bo0l_imu);
                         * continue;
                         * }*/
                    }
                    /*else
                     * {
                     *
                     * bo0l_imu.data = true;
                     * bool_pub.publish(bo0l_imu);
                     * break;
                     * }*/
                    // p4c. build joints array
                    SkeletonJoint aJoints[15];
                    aJoints[ 0] = rSkeleton.getJoint( JOINT_HEAD );
                    aJoints[ 1] = rSkeleton.getJoint( JOINT_NECK );
                    aJoints[ 2] = rSkeleton.getJoint( JOINT_LEFT_SHOULDER );
                    aJoints[ 3] = rSkeleton.getJoint( JOINT_RIGHT_SHOULDER );
                    aJoints[ 4] = rSkeleton.getJoint( JOINT_LEFT_ELBOW );
                    aJoints[ 5] = rSkeleton.getJoint( JOINT_RIGHT_ELBOW );
                    aJoints[ 6] = rSkeleton.getJoint( JOINT_LEFT_HAND );
                    aJoints[ 7] = rSkeleton.getJoint( JOINT_RIGHT_HAND );
                    aJoints[ 8] = rSkeleton.getJoint( JOINT_TORSO );
                    aJoints[ 9] = rSkeleton.getJoint( JOINT_LEFT_HIP );
                    aJoints[10] = rSkeleton.getJoint( JOINT_RIGHT_HIP );
                    aJoints[11] = rSkeleton.getJoint( JOINT_LEFT_KNEE );
                    aJoints[12] = rSkeleton.getJoint( JOINT_RIGHT_KNEE );
                    aJoints[13] = rSkeleton.getJoint( JOINT_LEFT_FOOT );
                    aJoints[14] = rSkeleton.getJoint( JOINT_RIGHT_FOOT );

                    // p4d. convert joint position to image
                    cv::Point2f aPoint[15];
                    for ( int s = 0; s < 15; ++ s )
                    {
                        const  Point3f & rPos = aJoints[s].getPosition();
                        mUserTracker.convertJointCoordinatesToDepth(
                                rPos.x, rPos.y, rPos.z,
                                &(aPoint[s].x), &(aPoint[s].y) );
                    }

                    // p4e. draw skeleton
                    /*cv::line( cImageBGR, aPoint[ 0], aPoint[ 1], cv::Scalar ( 0, ((i+1)%3)*255, ((i+1)%2)*255 ), 5 );
                     * cv::line( cImageBGR, aPoint[ 1], aPoint[ 2], cv::Scalar ( 0, ((i+1)%3)*255, ((i+1)%2)*255 ), 5 );
                     * cv::line( cImageBGR, aPoint[ 1], aPoint[ 3], cv::Scalar ( 0, ((i+1)%3)*255, ((i+1)%2)*255 ), 5 );
                     * cv::line( cImageBGR, aPoint[ 2], aPoint[ 4], cv::Scalar ( 0, ((i+1)%3)*255, ((i+1)%2)*255 ), 5 );
                     * cv::line( cImageBGR, aPoint[ 3], aPoint[ 5], cv::Scalar ( 0, ((i+1)%3)*255, ((i+1)%2)*255 ), 5 );
                     * cv::line( cImageBGR, aPoint[ 4], aPoint[ 6], cv::Scalar ( 0, ((i+1)%3)*255, ((i+1)%2)*255 ), 5 );
                     * cv::line( cImageBGR, aPoint[ 5], aPoint[ 7], cv::Scalar ( 0, ((i+1)%3)*255, ((i+1)%2)*255 ), 5 );
                     * cv::line( cImageBGR, aPoint[ 1], aPoint[ 8], cv::Scalar ( 0, ((i+1)%3)*255, ((i+1)%2)*255 ), 5 );
                     * cv::line( cImageBGR, aPoint[ 8], aPoint[ 9], cv::Scalar ( 0, ((i+1)%3)*255, ((i+1)%2)*255 ), 5 );
                     * cv::line( cImageBGR, aPoint[ 8], aPoint[10], cv::Scalar ( 0, ((i+1)%3)*255, ((i+1)%2)*255 ), 5 );
                     * cv::line( cImageBGR, aPoint[ 9], aPoint[10], cv::Scalar ( 0, ((i+1)%3)*255, ((i+1)%2)*255 ), 5 );
                     * cv::line( cImageBGR, aPoint[10], aPoint[12], cv::Scalar ( 0, ((i+1)%3)*255, ((i+1)%2)*255 ), 5 );
                     * cv::line( cImageBGR, aPoint[11], aPoint[13], cv::Scalar ( 0, ((i+1)%3)*255, ((i+1)%2)*255 ), 5 );
                     * cv::line( cImageBGR, aPoint[12], aPoint[14], cv::Scalar ( 0, ((i+1)%3)*255, ((i+1)%2)*255 ), 5 );*/

                    // p4f. draw joint
                    for ( int   s = 8; s < 9; ++ s )
                    {
                        //檢查各個關節點位置的可靠度，如果可靠度大於 0.5 的話，就用紅色畫，不然就用綠色
                        if(i==0)
                        {
                            if ( aJoints[s].getPositionConfidence() > 0.3 )
                                cv::circle( cImageBGR, aPoint[s], 12, cv::Scalar ( 0, 255, 0 ), 4 );
                        }
                        else
                        {
                            //if ( aJoints[s].getPositionConfidence() > 0.3 )
                            cv::circle( cImageBGR, aPoint[s], 12, cv::Scalar ( 0, 0, 255 ), 4 );
                        }
                    }

                }
            }
        }



        for ( int   k = 1; k <5;k++ ){  //512*424pixels

            int kk = 102.4;
            cv::line( cImageBGR, cv::Point(k*kk,53), cv::Point(k*kk,371), cv::Scalar ( 255, 0, 0 ), 2.5 );
             //cv::line( cImageBGR, cv::Point(k*kk,0), cv::Point(k*kk,480), cv::Scalar ( 255, 0, 0 ), 2.5 );
        }
        // p5. show image
        cv::imshow( "User Image" , cImageBGR );

        /*sensor_msgs::ImagePtr msg = cv_bridge::CvImage(std_msgs::Header(), "bgr8", cImageBGR).toImageMsg();
        image_pub_.publish(msg);*/

        ros::Time time = ros::Time::now();
          cv_ptr->encoding = "bgr8";
          cv_ptr->header.stamp = time;
          cv_ptr->header.frame_id = "/kinect_link";
          cv_ptr->image = cImageBGR;
          image_pub_.publish(cv_ptr->toImageMsg());


        // p6. check keyboard
        if ( cv ::waitKey( 1 ) == 'q' )

            ros::spinOnce();
        r.sleep();
    }

    // p7. stop
    mUserTracker.destroy();
    //mColorStream.destroy();
    //mDepthStream.destroy();
    //mDevice.close ();
    NiTE ::shutdown();
    OpenNI ::shutdown();

    return 0;

}
